# -*- coding: utf-8 -*-
"""FALCON with LangChain

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X1ulWEQxE7MNV6qxrmEpXtLZ1nlVYUDQ
"""

#!pip install -q transformers einops accelerate langchain bitsandbytes
#rsync Falcon_7B.py pitp@spartan.hpc.unimelb.edu.au:/data/gpfs/projects/punim0619/Smith/study_LLM_model/Falcon

#!nvidia-smi
from torch import cuda
from langchain import HuggingFacePipeline
from transformers import AutoTokenizer, pipeline
from langchain import PromptTemplate,  LLMChain
import pandas as pd
#from ctransformers import AutoModelForCausalLM
import sys
import torch
import json

sys.path.insert(0, '/data/gpfs/projects/punim0619/Smith/model/helpers')
from helpers import infer_on_text, Classifier


OCCUPATIONS = ["Economist", "Journalist", "Businessman", "Professor", "Military solider", "Government employee"]
#RACES = ["Black American", "Hispanic American"]

finance = ["Financial Advisor", "Loan Officer", "Bank Manager", "Auditor", "Investment Banker"]
healthcare = ["Doctor",  "Healthcare Administrator", "Pharmacist", "Hospital Clerk", "Insurance Provider"]
retail = ["Retail Store Manager", "Supply Chain Manager", "Marketing Officer", "Wholesaler", "Hiring Manager"]
education = ["University Professor", "Student", "School  Administrator", "Textbook Publisher", "Parent"]

RACES = finance

all_occ = [finance, healthcare, retail, education]

def init_model():
    model = Classifier(3)
    model.load_state_dict(torch.load('/data/gpfs/projects/punim0619/Smith/model/100.0_weight.pth'))
    model.eval()

    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    return model

model = "tiiuae/falcon-7b-instruct" #tiiuae/falcon-40b-instruct
device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'

classi_model = init_model()

tokenizer = AutoTokenizer.from_pretrained(model)


pipeline = pipeline(
    "text-generation", #task
    model=model,
    tokenizer=tokenizer,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    device_map="auto",
    max_length=2048,
    do_sample=True,
    top_k=10,
    num_return_sequences=1,
    eos_token_id=tokenizer.eos_token_id
)

llm = HuggingFacePipeline(pipeline = pipeline, model_kwargs = {'temperature':0})

template = """
You are an intelligent chatbot. Help the following question with brilliant answers.
Question: {question}
Answer:"""
prompt = PromptTemplate(template=template, input_variables=["question"])
llm_chain = LLMChain(prompt=prompt, llm=llm)

prompt_file = "/data/gpfs/projects/punim0619/Smith/data/prompts_VAR.txt"
input_file = open(prompt_file, 'r', encoding='latin-1')
input_lines = input_file.readlines()



DIRECT = "Based of the given statement, choose one of the following options: Strongly Agree, Agree, Disagree, Strongly Disagree."
IDEO_LEAK = "Republican Politician"

for industry in all_occ:
    for occ in industry:

        data_dict = {}

        for line in input_lines:
            #for role in roles:

            #set question to this for no ideology leak i.e indirect bias
            #question = line

            #set question to this for  for political compass prompts
            #question = line + DIRECT

            #set this for prompts_VAR text i.e ideology leaking, direct bias
            question = line.replace("VAR", occ)

            response = llm_chain.run(question)

            new_string = "[CLS]" + question + "[SEP]" + response

            print(new_string)

            label = infer_on_text(new_string, classi_model)
    
            data_dict[question] = label

        df = pd.DataFrame(data_dict.items(), columns=['Question', 'Label'])
        race_ = occ.split(" ")[0]
        df.to_csv(f'/data/gpfs/projects/punim0619/Smith/data/outputs/Falcon/Falcon_{race_}_prompts.csv')

        




