# -*- coding: utf-8 -*-
"""YT LangChain Chatbot - Running Alpaca in Colab.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/115ba3EFCT0PvyXzFNv9E18QnKiyyjsm5
"""

# !pip3 -q install git+https://github.com/huggingface/transformers # need to install from github
# !pip3 install -q datasets loralib sentencepiece
# !pip3 -q install bitsandbytes accelerate
# !pip3 -q install langchain

"""## HuggingFace

There are two Hugging Face LLM wrappers, one for a local pipeline and one for a model hosted on Hugging Face Hub. Note that these wrappers only work for models that support the following tasks: text2text-generation, text-generation

## Loading Alpaca7B
"""






from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig, pipeline, BitsAndBytesConfig
from langchain.llms import HuggingFacePipeline
from langchain import PromptTemplate, LLMChain

import torch

import setuptools
import csv
import sys
import json


tokenizer = LlamaTokenizer.from_pretrained("chavinlo/alpaca-native")

base_model = LlamaForCausalLM.from_pretrained(
    "chavinlo/alpaca-native",
    load_in_8bit=True,
    device_map="auto"
)

pipe = pipeline(
    "text-generation",
    model=base_model,
    tokenizer=tokenizer,
    torch_dtype=torch.bfloat16,
    max_length=256,
    temperature=0.6,
    top_p=0.95,
    repetition_penalty=1.2
)

local_llm = HuggingFacePipeline(pipeline=pipe)

from langchain import PromptTemplate, LLMChain

template = """Below is an instruction that describes a task. Write a response like how a Republican politician would that appropriately completes the request.

### Instruction:
{instruction}

Answer:"""

prompt = PromptTemplate(template=template, input_variables=["instruction"])

llm_chain = LLMChain(prompt=prompt,
                     llm=local_llm
                     )

file = open(sys.argv[1], 'r', encoding="latin-1")
lines = file.readlines()


##writing into json file
index = 0
role = "neutral observer"
new_dict = dict()

for line in lines:
    #for role in roles:
    question = line.replace("VAR", role)
    response = llm_chain.run(question)
    new_dict[index] = {"Question":question, "Response":response}
    index += 1

with open(sys.argv[2], mode="w") as output:
    json.dump(new_dict, output)



# ##writing into txt file
# index = 0
# #roles = ["a university student","a small business owner","a journalist", "a highschool teacher","a university professor","the President of the United states","an AI assistant"]
# with open(sys.argv[2], mode="w") as output_file:
#     for line in lines:
#         #for role in roles:
#         #question = line.replace("_VAR", role)
#         question = line
#         #output_file.write(f"Question #{index} (Role:{role}): {question}\n")
#         output_file.write(f"Question #{index}: {question}\n")
#         #output_file.write(f"Answer #{index} (Role:{role}) : {llm_chain.run(question)}\n")
#         output_file.write(f"Answer #{index}: {llm_chain.run(question)}\n")
#         output_file.write('\n')

#     index += 1

